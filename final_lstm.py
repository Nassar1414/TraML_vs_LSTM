# -*- coding: utf-8 -*-
"""Final_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-r3DX4FZF563mppyYHkR4suJQDjeJ9JK

**Step 1: Install Required Libraries**
"""

!pip install pandas scikit-learn tensorflow numpy matplotlib

"""**Step 2: Import Necessary Libraries**"""

# Import Essential Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import classification_report, confusion_matrix

df = pd.read_csv("/content/natural_disasters_2024.csv")
df

"""**Step 3: Mount Google Drive & Load Dataset**"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the Dataset
file_path = "/content/drive/MyDrive/Research/natural_disasters_2024.csv"  # Update path if needed
df = pd.read_csv(file_path)

# Display first few rows of the dataset
df.head()

"""**Step 4: Data Preprocessing**"""

# Convert 'Date' column to datetime format
df["Date"] = pd.to_datetime(df["Date"])

# Sort dataset by Date to maintain time-series order
df = df.sort_values(by="Date")
# Create Binary Target Column
df["Disaster_Occurred"] = 1  # Assume all rows have disasters
df.loc[df.duplicated(subset=["Date"]), "Disaster_Occurred"] = 0  # If duplicate date, assume no disaster

# Check class distribution
print(df["Disaster_Occurred"].value_counts())

# Normalize numerical columns using MinMaxScaler
scaler = MinMaxScaler()
df[["Magnitude", "Fatalities", "Economic_Loss($)"]] = scaler.fit_transform(
    df[["Magnitude", "Fatalities", "Economic_Loss($)"]])
# Confirm dataset structure after preprocessing
print("Dataset Columns:", df.columns)

# One-hot encode categorical columns if they exist
categorical_columns = ["Disaster_Type", "Location"]

if any(col in df.columns for col in categorical_columns):
    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
    print("Categorical features one-hot encoded.")

# Display updated column names
print("Updated columns after encoding:", df.columns)

"""**Step 5: Define Sequence Creation Function**"""

# Function to create sequences for LSTM input
def create_sequences(data, target_col, n_steps=30):
    X, y = [], []
    for i in range(len(data) - n_steps):
        X.append(data.iloc[i:i + n_steps].values)  # Collect past n_steps data
        y.append(data.iloc[i + n_steps][target_col])  # Corresponding label

    return np.array(X), np.array(y)

"""**Step 6: Balance Dataset with Synthetic Data**"""

# Balance Dataset with Synthetic Data (Before Splitting)

# Count existing classes
num_disasters = np.sum(df["Disaster_Occurred"] == 1)
num_no_disasters = np.sum(df["Disaster_Occurred"] == 0)

print(f"Before Balancing: Disasters = {num_disasters}, No Disasters = {num_no_disasters}")

# Generate Synthetic "No Disaster" Samples If Missing
if num_no_disasters == 0:
    print(" No 'No Disaster' (0s) found! Adding synthetic samples...")

    noise_factor = 0.05
    X_no_disaster = df.drop(columns=["Date", "Disaster_ID"]) + noise_factor * np.random.randn(*df.drop(columns=["Date", "Disaster_ID"]).shape)
    y_no_disaster = np.zeros(X_no_disaster.shape[0])  # Label as "No Disaster"

    # Combine real & synthetic data
    df_balanced = pd.concat([df, pd.DataFrame(X_no_disaster, columns=df.drop(columns=["Date", "Disaster_ID"]).columns)])
    df_balanced["Disaster_Occurred"] = np.hstack((df["Disaster_Occurred"], y_no_disaster))

    print(f"After Balancing: Disasters = {df_balanced['Disaster_Occurred'].sum()}, No Disasters = {np.sum(df_balanced['Disaster_Occurred'] == 0)}")
else:
    df_balanced = df.copy()  # No need to modify if balanced

"""**Step 7: Prepare Data for LSTM**"""

# Prepare Data for LSTM (After Balancing)
n_steps = 30

# Create sequences for LSTM model
X, y = create_sequences(df_balanced.drop(columns=["Date", "Disaster_ID"]), "Disaster_Occurred", n_steps)

# Split dataset into training & testing sets (Stratified to Maintain Balance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Display class distribution after split
unique_train, counts_train = np.unique(y_train, return_counts=True)
unique_test, counts_test = np.unique(y_test, return_counts=True)

print(f"After Split: Training Set: {dict(zip(unique_train, counts_train))}")
print(f"After Split: Test Set: {dict(zip(unique_test, counts_test))}")

"""**Step 8: Feature Selection Using Mutual Information Scores**


"""

# Flatten the time-series data for feature selection
X_flat = X_train.reshape(X_train.shape[0], -1)

# Convert X_flat to float (to ensure it can be checked for NaN/Inf values)
X_flat = X_flat.astype(np.float64)

# Debugging X_flat before feeding into Random Forest
print("X_flat shape:", X_flat.shape)

# Check if X_flat contains only zeros
print("Any nonzero values in X_flat?", np.any(X_flat != 0))

# Check for NaN or Infinite values
print("Any NaN values in X_flat?", np.isnan(X_flat).any())
print("Any Infinite values in X_flat?", np.isinf(X_flat).any())

# Check feature-wise variance (shouldn't be zero)
feature_variance = np.var(X_flat, axis=0)
print("Feature Variance (first 20 features):", feature_variance[:20])  # Print first 20 for reference
print("Number of features with zero variance:", np.sum(feature_variance == 0))

# Debugging X_flat before feeding into Random Forest
print("X_flat shape:", X_flat.shape)

# Check if X_flat contains only zeros
print("Any nonzero values in X_flat?", np.any(X_flat != 0))

# Check for NaN or Infinite values
print("Any NaN values in X_flat?", np.isnan(X_flat).any())
print("Any Infinite values in X_flat?", np.isinf(X_flat).any())

# Check feature-wise variance (shouldn't be zero)
feature_variance = np.var(X_flat, axis=0)
print("Feature Variance (first 20 features):", feature_variance[:20])  # Print first 20 for reference
print("Number of features with zero variance:", np.sum(feature_variance == 0))

# Identify zero-variance features
zero_variance_features = np.where(feature_variance == 0)[0]
print("Removing these zero-variance features:", zero_variance_features)

# Remove them from X_flat
X_flat = np.delete(X_flat, zero_variance_features, axis=1)

# Print new shape after removal
print("New X_flat shape:", X_flat.shape)

# Compute Mutual Information Scores
mi_scores = mutual_info_classif(X_flat, y_train)
sorted_idx = np.argsort(mi_scores)[::-1]

# Print top 10 features
print("Top 10 most important features based on Mutual Information:")
for i in range(10):
    print(f"Feature {sorted_idx[i]} - Score: {mi_scores[sorted_idx[i]]:.6f}")

# Select features with MI score above a small threshold (e.g., 0.01)
threshold = 0.0002
selected_features = sorted_idx[mi_scores > threshold]

# Apply the selection
X_flat_selected = X_flat[:, selected_features]

print("Reduced feature set size:", X_flat_selected.shape[1])
print("Reduced feature set size:", len(selected_features))
print("Min MI score of selected features:", np.min(mi_scores[selected_features]))
print("Max MI score of selected features:", np.max(mi_scores[selected_features]))

# Save selected features
feature_save_path = "/content/drive/MyDrive/Research/selected_features.pkl"
with open(feature_save_path, "wb") as f:
    pickle.dump(selected_features, f)

print("Selected features saved successfully!")

"""**Step 9: Build & Compile LSTM Model**"""

# Define the LSTM model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(n_steps, X_train.shape[2]), kernel_regularizer=l2(0.03), recurrent_dropout=0.2),
    BatchNormalization(),
    Dropout(0.5),
    LSTM(32, return_sequences=False, kernel_regularizer=l2(0.03)),
    BatchNormalization(),
    Dropout(0.5),
    Dense(1, activation="sigmoid", kernel_regularizer=l2(0.03))
])

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)
model.compile(optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy"])

# Convert X_train and X_test to float32
X_train = np.array(X_train, dtype=np.float32)
X_test = np.array(X_test, dtype=np.float32)

# Convert y_train and y_test to float32
y_train = np.array(y_train, dtype=np.float32)
y_test = np.array(y_test, dtype=np.float32)

# Check for invalid types
print("X_train dtype:", X_train.dtype)
print("X_test dtype:", X_test.dtype)
print("y_train dtype:", y_train.dtype)
print("y_test dtype:", y_test.dtype)

"""**Step 10: Train Model with Early Stopping and save the model**"""

early_stopping = EarlyStopping(monitor="val_accuracy", patience=10, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Save the trained model
model.save("/content/drive/MyDrive/Research/disaster_prediction_model.h5")

print("Model saved successfully!")

"""**Step 11: Evaluate Model**"""

y_pred = (model.predict(X_test) > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Model Performance:\n Accuracy: {accuracy:.4f} \n Precision: {precision:.4f} \n Recall: {recall:.4f} \n F1 Score: {f1:.4f}")

"""**Step 12: Plot Model Training History**"""

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Loss Over Epochs")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Val Accuracy")
plt.title("Accuracy Over Epochs")
plt.legend()
plt.show()

# Check Class Distribution in Training & Test Data
unique_train, counts_train = np.unique(y_train, return_counts=True)
unique_test, counts_test = np.unique(y_test, return_counts=True)
print("Checking Model Performance & Possible Issues")
print(f"Class Distribution in Training Data: {dict(zip(unique_train, counts_train))}")
print(f"Class Distribution in Test Data: {dict(zip(unique_test, counts_test))}")

# Get Predictions
y_pred = (model.predict(X_test) > 0.5).astype(int)
unique_pred, counts_pred = np.unique(y_pred, return_counts=True)
print(f"\nPredictions Distribution (y_pred): {dict(zip(unique_pred, counts_pred))}")

# Compute Loss & Accuracy
train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
val_loss, val_acc = model.evaluate(X_test, y_test, verbose=0)

print(f"Final Training Accuracy: {train_acc:.4f}")
print(f"Final Validation Accuracy: {val_acc:.4f}")
print(f"Final Training Loss: {train_loss:.4f}")
print(f"Final Validation Loss: {val_loss:.4f}\n")

# Display Classification Report
print(" Classification Report:\n")
print(classification_report(y_test, y_pred, digits=4))

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["No Disaster", "Disaster"], yticklabels=["No Disaster", "Disaster"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("\n Done! Analyze results above & adjust model if needed.")

from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_accuracies = []

for train_index, val_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)
    y_val_pred = (model.predict(X_val_fold) > 0.5).astype(int)

    fold_accuracy = accuracy_score(y_val_fold, y_val_pred)
    cv_accuracies.append(fold_accuracy)

print(f"\n Cross-Validation Accuracy (Mean): {np.mean(cv_accuracies) * 100:.2f}%")

import numpy as np
from sklearn.preprocessing import MinMaxScaler


np.random.seed(42)
test_data = np.random.rand(50, 13)


scaler = MinMaxScaler()
test_data_scaled = scaler.fit_transform(test_data)


time_steps = 30
features = 13


X_test = []
for i in range(len(test_data_scaled) - time_steps):
    X_test.append(test_data_scaled[i : i + time_steps, :features])

X_test = np.array(X_test)


print(f" X_test shape: {X_test.shape}")


predictions = model.predict(X_test)

predictions_original = scaler.inverse_transform(
    np.concatenate((np.zeros((predictions.shape[0], features-1)), predictions), axis=1)
)[:, -1]


print(" Predicted values:", predictions_original[:10])

import numpy as np
from sklearn.preprocessing import MinMaxScaler


np.random.seed(42)
test_data = np.random.rand(50, 13)


scaler = MinMaxScaler()
test_data_scaled = scaler.fit_transform(test_data)


time_steps = 30
features = 13


X_test = []
for i in range(len(test_data_scaled) - time_steps):
    X_test.append(test_data_scaled[i : i + time_steps, :features])

X_test = np.array(X_test)

print(f" X_test shape: {X_test.shape}")


predictions = model.predict(X_test)


predictions_original = scaler.inverse_transform(
    np.concatenate((np.zeros((predictions.shape[0], features-1)), predictions), axis=1)
)[:, -1]


threshold = 0.5
predicted_labels = ["Disaster" if p >= threshold else "No Disaster" for p in predictions_original]

print(" Predicted Categories:", predicted_labels[:10])